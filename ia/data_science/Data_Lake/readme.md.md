
# ðŸ—‚ï¸ Data Lake - Gym Master

## ðŸ“Œ DescripciÃ³n

Este Data Lake centraliza y consolida la informaciÃ³n de **todos los gimnasios** gestionados en Gym Master. La arquitectura estÃ¡ diseÃ±ada para soportar:
- AnÃ¡lisis globales y comparativas entre gimnasios.
- Modelos de Machine Learning centralizados.
- Dashboards y reporting avanzados.

La estructura sigue un enfoque en **capas**, actualmente implementamos la capa **Processed** con datos limpios y unificados.

---

## ðŸ“‚ Estructura de Carpetas

```
ia/data_science/Data_Lake/Processed/
â”‚
â”œâ”€â”€ usuarios/
â”‚    â””â”€â”€ {gimnasio}_usuarios.parquet
â”‚
â”œâ”€â”€ asistencias/
â”‚    â””â”€â”€ {gimnasio}_asistencias.parquet
â”‚
â”œâ”€â”€ rutinas/
â”‚    â””â”€â”€ {gimnasio}_rutinas.parquet
â”‚
â””â”€â”€ logs_uso/
     â””â”€â”€ {gimnasio}_logs_uso.parquet
```

Cada archivo Parquet lleva el prefijo del gimnasio correspondiente.

---

## ðŸ·ï¸ Esquema de Datos

### usuarios
| campo     | tipo    |
|-----------|---------|
| socio_id  | UUID    |
| gimnasio  | texto   |
| email     | texto   |
| sexo      | M/F     |
| edad      | int     |
| nivel     | texto   |
| objetivo  | texto   |
| rol       | texto   |
| activo    | bool    |
| creado_en | timestamp |

---

### asistencias
| campo         | tipo    |
|---------------|---------|
| asistencia_id | UUID    |
| socio_id      | UUID    |
| gimnasio      | texto   |
| fecha         | date    |
| dia_semana    | texto   |
| hora_ingreso  | time    |
| hora_egreso   | time    |
| creado_en     | timestamp |

---

### rutinas
| campo     | tipo    |
|-----------|---------|
| rutina_id | UUID    |
| socio_id  | UUID    |
| gimnasio  | texto   |
| semana    | int     |
| nombre    | texto   |
| contenido | JSON    |
| creado_en | timestamp |

---

### logs_uso
| campo     | tipo    |
|-----------|---------|
| log_id    | UUID    |
| gimnasio  | texto   |
| usuario_id| UUID    |
| accion    | texto   |
| detalle   | texto   |
| fecha_hora| timestamp |

---

## âš™ï¸ Pipelines Disponibles

### 1. Crear estructura Parquet vacÃ­a
```
python ia/data_science/Setup/setup_data_lake.py
```

### 2. Cargar datos al Data Lake
```
python ia/data_science/Pipelines/pipeline_carga_data_lake.py
```

> Este pipeline toma los datos del ETL, los unifica y los guarda en las carpetas correspondientes.

---

## ðŸš€ PrÃ³ximos pasos

- Implementar capa **Raw** para almacenamiento de datos crudos.
- Particionar Parquet por fechas o gimnasio para optimizar queries.
- Integrar logs de uso reales.
- Implementar versiones en la capa **Aggregated** para modelos ML.

---

## ðŸ“… Actualizado por
**Octavio Alvarez** - Julio 2025
